ccopy_reg
_reconstructor
p1
(c__main__
LinearRegression
p2
c__builtin__
object
p3
NtRp4
(dp5
S'b'
g1
(ctheano.tensor.sharedvar
TensorSharedVariable
p6
g3
NtRp7
(dp8
S'auto_name'
p9
S'auto_11'
p10
sS'index'
p11
NsS'tag'
p12
(itheano.gof.utils
scratchpad
p13
(dp14
S'trace'
p15
(lp16
(lp17
(S'/Users/ray/git/dl_exp/deeplearning/code/linear_regressor.py'
p18
I433
S'<module>'
p19
S'sgd_optimization_mnist()'
tp20
a(S'/Users/ray/git/dl_exp/deeplearning/code/linear_regressor.py'
p21
I288
S'sgd_optimization_mnist'
p22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp23
a(S'/Users/ray/git/dl_exp/deeplearning/code/linear_regressor.py'
p24
I86
S'__init__'
p25
S"self.b = theano.shared(numpy.asarray(0, dtype=theano.config.floatX), name = 'b', borrow = True)"
tp26
aasbsS'container'
p27
g1
(ctheano.gof.link
Container
p28
g3
NtRp29
(dp30
S'name'
p31
S'b'
sS'storage'
p32
(lp33
cnumpy.core.multiarray
_reconstruct
p34
(cnumpy
ndarray
p35
(I0
tS'b'
tRp36
(I1
(tcnumpy
dtype
p37
(S'f4'
I0
I1
tRp38
(I3
S'<'
NNNI-1
I-1
I0
tbI00
S'X\x00\xbaA'
tbasS'strict'
p39
I00
sS'readonly'
p40
I00
sS'type'
p41
g1
(ctheano.tensor.type
TensorType
p42
g3
NtRp43
(dp44
S'broadcastable'
p45
(tsS'dtype'
p46
S'float32'
p47
sS'numpy_dtype'
p48
g38
sS'sparse_grad'
p49
I00
sg31
NsbsS'allow_downcast'
p50
Nsbsg31
S'b'
sS'owner'
p51
Nsg41
g43
sbsS'y_pred'
p52
g1
(ctheano.tensor.var
TensorVariable
p53
g3
NtRp54
(dp55
g9
S'auto_17'
p56
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p57
(dp58
g15
(lp59
(lp60
(g18
I433
g19
S'sgd_optimization_mnist()'
tp61
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp62
a(g24
I96
g25
S'self.p_y_given_x = T.dot(input, self.W) + self.b'
tp63
aasbsg31
Nsg51
g1
(ctheano.gof.graph
Apply
p64
g3
NtRp65
(dp66
S'inputs'
p67
(lp68
g1
(g53
g3
NtRp69
(dp70
g9
S'auto_12'
p71
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p72
(dp73
g15
(lp74
(lp75
(g18
I433
g19
S'sgd_optimization_mnist()'
tp76
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp77
a(g24
I96
g25
S'self.p_y_given_x = T.dot(input, self.W) + self.b'
tp78
aasbsg31
Nsg51
g1
(g64
g3
NtRp79
(dp80
g67
(lp81
g1
(g53
g3
NtRp82
(dp83
g9
S'auto_8'
p84
sg11
Nsg12
(itheano.gof.utils
scratchpad
p85
(dp86
g15
(lp87
(lp88
(g18
I433
g19
S'sgd_optimization_mnist()'
tp89
a(g21
I283
g22
S"x = T.matrix('x')  # data, presented as rasterized images"
tp90
aasbsg31
S'x'
sg51
Nsg41
g1
(g42
g3
NtRp91
(dp92
g45
(I00
I00
tp93
sg46
S'float32'
p94
sg48
g38
sg49
I00
sg31
Nsbsbag1
(g6
g3
NtRp95
(dp96
g9
S'auto_10'
p97
sg11
Nsg12
(itheano.gof.utils
scratchpad
p98
(dp99
g15
(lp100
(lp101
(g18
I433
g19
S'sgd_optimization_mnist()'
tp102
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp103
a(g24
I84
g25
S"name = 'W', borrow = True)"
tp104
aasbsg27
g1
(g28
g3
NtRp105
(dp106
g31
S'W'
sg32
(lp107
g34
(g35
(I0
tS'b'
tRp108
(I1
(I13
tg37
(S'f8'
I0
I1
tRp109
(I3
S'<'
NNNI-1
I-1
I0
tbI00
S'{\x9dF\x00\xeb\xe5\x16\xc0\xcc\xf0\x13Q-\xb6\xc2?\x7fa\x95\x8b\xc0+\xf1?\x17Eo\x0f\x97\xbd\xeb?\xcb\xb1\xa8Z\x9ep\x01\xc0"N\xd20~\x07\xca?\x80c\xd4\xbf\x03X\xc1?\x8cA\x0e\xe7\xa9\xbb\x02\xc0w\xc7\xe5\x8b?C\x19@\xbc\xf1-\xdb\xac\xcb\xef\xbf\xd2}\xae\xbe\x8aZ\xde\xbf\xee-\x80c-\x92\x9d?NL\xaa\xe8\x01\x1d\x16\xc0'
tbasg39
I00
sg40
I00
sg41
g1
(g42
g3
NtRp110
(dp111
g45
(I00
tp112
sg46
S'float64'
p113
sg48
g109
sg49
I00
sg31
Nsbsg50
Nsbsg31
S'W'
sg51
Nsg41
g110
sbasg12
(itheano.gof.utils
scratchpad
p114
(dp115
bsS'outputs'
p116
(lp117
g69
asS'op'
p118
g1
(ctheano.tensor.basic
Dot
p119
g3
NtRp120
(dp121
S'_op_use_c_code'
p122
S'/usr/bin/clang++'
p123
sbsbsg41
g1
(g42
g3
NtRp124
(dp125
g45
(I00
tp126
sg46
S'float64'
p127
sg48
g109
sg49
I00
sg31
Nsbsbag1
(g53
g3
NtRp128
(dp129
g9
S'auto_16'
p130
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p131
(dp132
g15
(lp133
(lp134
(g18
I433
g19
S'sgd_optimization_mnist()'
tp135
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp136
a(g24
I96
g25
S'self.p_y_given_x = T.dot(input, self.W) + self.b'
tp137
aasbsg31
Nsg51
g1
(g64
g3
NtRp138
(dp139
g67
(lp140
g7
asg12
(itheano.gof.utils
scratchpad
p141
(dp142
bsg116
(lp143
g128
asg118
g1
(ctheano.tensor.elemwise
DimShuffle
p144
g3
NtRp145
(dp146
S'drop'
p147
(lp148
sS'shuffle'
p149
(lp150
sS'augment'
p151
(lp152
I0
asS'input_broadcastable'
p153
(tsS'inplace'
p154
I00
sS'new_order'
p155
(S'x'
tp156
sg122
g123
sbsbsg41
g1
(g42
g3
NtRp157
(dp158
g45
(I01
tp159
sg46
g47
sg48
g38
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p160
(dp161
bsg116
(lp162
g54
asg118
g1
(ctheano.tensor.elemwise
Elemwise
p163
g3
NtRp164
(dp165
S'__module__'
p166
S'tensor'
p167
sS'scalar_op'
p168
g1
(ctheano.scalar.basic
Add
p169
g3
NtRp170
(dp171
S'output_types_preference'
p172
ctheano.scalar.basic
upcast_out
p173
sg122
g123
sg31
S'add'
p174
sbsg31
S'Elemwise{add,no_inplace}'
p175
sg122
g123
sS'destroy_map'
p176
(dp177
sS'nfunc_spec'
p178
(S'add'
I2
I1
tp179
sS'inplace_pattern'
p180
(dp181
sS'openmp'
p182
I00
sS'__doc__'
p183
S"elementwise addition\n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Examples\n    --------\n    Elemwise(add) # represents + on tensors (x + y)\n    Elemwise(add, {0 : 0}) # represents the += operation (x += y)\n    Elemwise(add, {0 : 1}) # represents += on the second argument (y += x)\n    Elemwise(mul)(rand(10, 5), rand(1, 5)) # the second input is completed\n    # along the first dimension to match the first input\n    Elemwise(true_div)(rand(10, 5), rand(10, 1)) # same but along the\n    # second dimension\n    Elemwise(int_div)(rand(1, 5), rand(10, 1)) # the output has size (10, 5)\n    Elemwise(log)(rand(3, 4, 5))\n\n    "
p184
sbsbsg41
g1
(g42
g3
NtRp185
(dp186
g45
(I00
tp187
sg46
S'float64'
p188
sg48
g109
sg49
I00
sg31
NsbsbsS'l2'
p189
F0.0001
sS'l1'
p190
F0.0001
sS'reg_param_L2'
p191
g1
(g53
g3
NtRp192
(dp193
g9
S'auto_38'
p194
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p195
(dp196
g15
(lp197
(lp198
(g18
I433
g19
S'sgd_optimization_mnist()'
tp199
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp200
a(g24
I112
g25
S'self.reg_param_L2 = T.sum(T.sqr(self.W)) + T.sum(T.sqr(self.b))'
tp201
aasbsg31
Nsg51
g1
(g64
g3
NtRp202
(dp203
g67
(lp204
g1
(g53
g3
NtRp205
(dp206
g9
S'auto_30'
p207
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p208
(dp209
g15
(lp210
(lp211
(g18
I433
g19
S'sgd_optimization_mnist()'
tp212
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp213
a(g24
I112
g25
S'self.reg_param_L2 = T.sum(T.sqr(self.W)) + T.sum(T.sqr(self.b))'
tp214
aasbsg31
Nsg51
g1
(g64
g3
NtRp215
(dp216
g67
(lp217
g1
(g53
g3
NtRp218
(dp219
g9
S'auto_29'
p220
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p221
(dp222
g15
(lp223
(lp224
(g18
I433
g19
S'sgd_optimization_mnist()'
tp225
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp226
a(g24
I112
g25
S'self.reg_param_L2 = T.sum(T.sqr(self.W)) + T.sum(T.sqr(self.b))'
tp227
aasbsg31
Nsg51
g1
(g64
g3
NtRp228
(dp229
g67
(lp230
g95
asg12
(itheano.gof.utils
scratchpad
p231
(dp232
bsg116
(lp233
g218
asg118
g1
(g163
g3
NtRp234
(dp235
g166
g167
sg168
g1
(ctheano.scalar.basic
Sqr
p236
g3
NtRp237
(dp238
g172
ctheano.scalar.basic
same_out
p239
sg122
g123
sg31
S'sqr'
p240
sbsg31
S'Elemwise{sqr,no_inplace}'
p241
sg122
g123
sg176
(dp242
sg178
(S'square'
p243
I1
I1
tp244
sg180
(dp245
sg182
I00
sg183
S"square of a\n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Examples\n    --------\n    Elemwise(add) # represents + on tensors (x + y)\n    Elemwise(add, {0 : 0}) # represents the += operation (x += y)\n    Elemwise(add, {0 : 1}) # represents += on the second argument (y += x)\n    Elemwise(mul)(rand(10, 5), rand(1, 5)) # the second input is completed\n    # along the first dimension to match the first input\n    Elemwise(true_div)(rand(10, 5), rand(10, 1)) # same but along the\n    # second dimension\n    Elemwise(int_div)(rand(1, 5), rand(10, 1)) # the output has size (10, 5)\n    Elemwise(log)(rand(3, 4, 5))\n\n    "
p246
sbsbsg41
g1
(g42
g3
NtRp247
(dp248
g45
(I00
tp249
sg46
g188
sg48
g109
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p250
(dp251
bsg116
(lp252
g205
asg118
g1
(ctheano.tensor.elemwise
Sum
p253
g3
NtRp254
(dp255
S'acc_dtype'
p256
g188
sg46
g188
sg122
g123
sg168
g170
sS'axis'
p257
Nsbsbsg41
g1
(g42
g3
NtRp258
(dp259
g45
(tsg46
g188
sg48
g109
sg49
I00
sg31
Nsbsbag1
(g53
g3
NtRp260
(dp261
g9
S'auto_34'
p262
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p263
(dp264
g15
(lp265
(lp266
(g18
I433
g19
S'sgd_optimization_mnist()'
tp267
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp268
a(g24
I112
g25
S'self.reg_param_L2 = T.sum(T.sqr(self.W)) + T.sum(T.sqr(self.b))'
tp269
aasbsg31
Nsg51
g1
(g64
g3
NtRp270
(dp271
g67
(lp272
g1
(g53
g3
NtRp273
(dp274
g9
S'auto_33'
p275
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p276
(dp277
g15
(lp278
(lp279
(g18
I433
g19
S'sgd_optimization_mnist()'
tp280
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp281
a(g24
I112
g25
S'self.reg_param_L2 = T.sum(T.sqr(self.W)) + T.sum(T.sqr(self.b))'
tp282
aasbsg31
Nsg51
g1
(g64
g3
NtRp283
(dp284
g67
(lp285
g7
asg12
(itheano.gof.utils
scratchpad
p286
(dp287
bsg116
(lp288
g273
asg118
g234
sbsg41
g1
(g42
g3
NtRp289
(dp290
g45
(tsg46
S'float32'
p291
sg48
g38
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p292
(dp293
bsg116
(lp294
g260
asg118
g1
(g253
g3
NtRp295
(dp296
g256
g188
sg46
g291
sg122
g123
sg168
g170
sg257
Nsbsbsg41
g1
(g42
g3
NtRp297
(dp298
g45
(tsg46
g291
sg48
g38
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p299
(dp300
bsg116
(lp301
g192
asg118
g164
sbsg41
g1
(g42
g3
NtRp302
(dp303
g45
(tsg46
g188
sg48
g109
sg49
I00
sg31
NsbsbsS'reg_param_L1'
p304
g1
(g53
g3
NtRp305
(dp306
g9
S'auto_26'
p307
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p308
(dp309
g15
(lp310
(lp311
(g18
I433
g19
S'sgd_optimization_mnist()'
tp312
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp313
a(g24
I111
g25
S'self.reg_param_L1  = abs(T.sum(self.W) + T.sum(self.b))'
tp314
aasbsg31
Nsg51
g1
(g64
g3
NtRp315
(dp316
g67
(lp317
g1
(g53
g3
NtRp318
(dp319
g9
S'auto_23'
p320
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p321
(dp322
g15
(lp323
(lp324
(g18
I433
g19
S'sgd_optimization_mnist()'
tp325
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp326
a(g24
I111
g25
S'self.reg_param_L1  = abs(T.sum(self.W) + T.sum(self.b))'
tp327
aasbsg31
Nsg51
g1
(g64
g3
NtRp328
(dp329
g67
(lp330
g1
(g53
g3
NtRp331
(dp332
g9
S'auto_18'
p333
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p334
(dp335
g15
(lp336
(lp337
(g18
I433
g19
S'sgd_optimization_mnist()'
tp338
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp339
a(g24
I111
g25
S'self.reg_param_L1  = abs(T.sum(self.W) + T.sum(self.b))'
tp340
aasbsg31
Nsg51
g1
(g64
g3
NtRp341
(dp342
g67
(lp343
g95
asg12
(itheano.gof.utils
scratchpad
p344
(dp345
bsg116
(lp346
g331
asg118
g1
(g253
g3
NtRp347
(dp348
g256
g113
sg46
g113
sg122
g123
sg168
g170
sg257
Nsbsbsg41
g1
(g42
g3
NtRp349
(dp350
g45
(tsg46
g113
sg48
g109
sg49
I00
sg31
Nsbsbag1
(g53
g3
NtRp351
(dp352
g9
S'auto_19'
p353
sg11
I0
sg12
(itheano.gof.utils
scratchpad
p354
(dp355
g15
(lp356
(lp357
(g18
I433
g19
S'sgd_optimization_mnist()'
tp358
a(g21
I288
g22
S'classifier = LinearRegression(input=x, n_in=train_set_x.get_value(borrow=True).shape[1], n_out=1)'
tp359
a(g24
I111
g25
S'self.reg_param_L1  = abs(T.sum(self.W) + T.sum(self.b))'
tp360
aasbsg31
Nsg51
g1
(g64
g3
NtRp361
(dp362
g67
(lp363
g7
asg12
(itheano.gof.utils
scratchpad
p364
(dp365
bsg116
(lp366
g351
asg118
g1
(g253
g3
NtRp367
(dp368
g256
g188
sg46
g47
sg122
g123
sg168
g170
sg257
Nsbsbsg41
g1
(g42
g3
NtRp369
(dp370
g45
(tsg46
g47
sg48
g38
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p371
(dp372
bsg116
(lp373
g318
asg118
g164
sbsg41
g1
(g42
g3
NtRp374
(dp375
g45
(tsg46
g188
sg48
g109
sg49
I00
sg31
Nsbsbasg12
(itheano.gof.utils
scratchpad
p376
(dp377
bsg116
(lp378
g305
asg118
g1
(g163
g3
NtRp379
(dp380
g166
g167
sg168
g1
(ctheano.scalar.basic
Abs
p381
g3
NtRp382
(dp383
g172
g239
sg122
g123
sg31
Nsbsg31
S'Elemwise{abs_,no_inplace}'
p384
sg122
g123
sg176
(dp385
sg178
(S'abs'
I1
I1
tp386
sg180
(dp387
sg182
I00
sg183
S"|`a`|\n\n    TensorVariable overloads the `TensorVariable.__abs__` operator so that\n    this function is called when you type abs(a).\n\n    \n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Examples\n    --------\n    Elemwise(add) # represents + on tensors (x + y)\n    Elemwise(add, {0 : 0}) # represents the += operation (x += y)\n    Elemwise(add, {0 : 1}) # represents += on the second argument (y += x)\n    Elemwise(mul)(rand(10, 5), rand(1, 5)) # the second input is completed\n    # along the first dimension to match the first input\n    Elemwise(true_div)(rand(10, 5), rand(10, 1)) # same but along the\n    # second dimension\n    Elemwise(int_div)(rand(1, 5), rand(10, 1)) # the output has size (10, 5)\n    Elemwise(log)(rand(3, 4, 5))\n\n    "
p388
sbsbsg41
g1
(g42
g3
NtRp389
(dp390
g45
(tsg46
g188
sg48
g109
sg49
I00
sg31
NsbsbsS'params'
p391
(lp392
g95
ag7
asS'W'
g95
sS'input'
p393
g82
sS'p_y_given_x'
p394
g54
sb.